# -*- coding: utf-8 -*-
"""Yet another copy of Prediction of Drinkers using Body Signals

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R5__YN9f--0e3KKFZIeMneqE7NsRXQDx
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING ='data4455:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4933586%2F8305040%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240503%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240503T193438Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D13e3d6d6b6c16aa21365b4c9eb9b001b585b6246d9d4fb69020983321f1e3e122f3f2b3ee6d7133dadfaecbc9e4799105c78851843962ddf4a8ccf5f882362f3e53c389bada33702fce0521d3056787a137a5c01d663d8241425f92777372a60a465fa3c9d14957a482f848fa02f0b6cd72d78c6b6c5dd2b56e421f6fa687332f42963b60623ebeca8a2630a34450c3e8d17bc34c8e7c6a48a2a0267617f517e60fec030bb4745bc6882bfeac0e047fee673eb0b2366654b4db12fe07d612e31269ee5a89b8ad7e7dece76a795112e8c958a2fc23d7d3ee06a15e76ab11ef7f1029779f44539b5349fce73731957799212c416abf40c4e1e5a18de9f8fe8a84c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt

"""# 1. Import necessary libraries"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

"""# 2. Load the dataset"""

df = pd.read_csv("/content/drive/MyDrive/dataset/smoking_driking_dataset_Ver01.csv")

"""# 3. Basic Data Exploration

**Display DataFrame**
"""

df.head()

df.shape

from google.colab import drive
drive.mount('/content/drive')

"""**Dataset Shape**

**Statistics**
"""

df.describe()

"""**Dataset Information**"""

df.info()

"""# 4. Data Preprocessing

**Missing Values**
"""

df.isnull().sum()

df.head()

"""**Label Encoding**"""

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

categorical_columns = ['sex','DRK_YN']

# Apply label encoding to each categorical column

for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])

df.head()

"""**Define features and the target variable**"""

X = df.drop(['DRK_YN'], axis=1) #Features

y = df['DRK_YN']  #Target variable

df.isnull().sum().sum()



"""**Distribution of Age**

# 6. Data Splitting
"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

"""# 7. MODEL BUILDING"""

# from sklearn.linear_model import LogisticRegression


# model = LogisticRegression()
from xgboost import XGBClassifier

# Initialize the XGBoost classifier
model = XGBClassifier()

"""**Model Selection**"""

model.fit(X_train, y_train)

"""**Model Training**"""

# Make predictions on the test set

y_pred = model.predict(X_test)

"""# 8. EVALUATION"""

from sklearn.metrics import accuracy_score, classification_report

"""**Evaluate the model**"""

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")

"""**Classification Report**"""

print(classification_report(y_test, y_pred))

"""# 9. CONFUSION MATRIX"""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 6))


sns.heatmap(
    cm, annot=True, fmt='d', cmap= 'Blues', linewidths=0.4, square=True, cbar=True,
    xticklabels=["N", "Y"],
    yticklabels=["N", "Y"]
)

plt.xlabel('Predicted', fontsize=14, fontweight='bold')
plt.ylabel('Actual', fontsize=14, fontweight='bold')
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.yticks(rotation=360)

plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
preprocessing.LabelEncoder()
import random
import numpy as np
import warnings
warnings.filterwarnings("ignore")
import pandas as pd

def calaculate_accuracy(x):
    X=df[x]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.30)
    lgt_R = XGBClassifier(solver='lbfgs')
    lgt_R.fit(X_train, y_train)
    accuracy=lgt_R.score(X_test,y_test)
    print('Accuracy:',accuracy)
    return accuracy

columnsName=df.drop(labels= 'DRK_YN', axis= 1).columns.values.tolist()

print(columnsName)

columnsName1=[0,1]
chromosomes=[]
for i in range(10):
    chro1=[]
    for i in range(23):
        item = random.choice(tuple(columnsName1))
        chro1.append(item)
    chromosomes.append(chro1)

def data(chromosomes1):
    chromosomes2=[]
    for i in range(len(chromosomes1)):
      if chromosomes1[i]==1:
          chromosomes2.append(columnsName[i])
    return chromosomes2

pb=[]
def checkpersonalnest():
  for i in range(len(chromosomes)):
    pb.append(calaculate_accuracy(data(chromosomes[i])))
checkpersonalnest()

def checkvelocity(globalbest):
    velocity=[]
    for j in range(len(chromosomes)):
        velocity.append(list(0+1*(np.random.random(1)[0])*(np.array(chromosomes[j])-np.array(chromosomes[j]))+1*(np.random.random(1)[0])*(np.array(globalbest)-np.array(chromosomes[j]))))
    #print(velocity)
    return velocity

def addingchromosomes(velocity):
    chromosomes2=[]
    for i in range(len(velocity)):
        nextchromo=[]
        for j in range(len(velocity[i])):
            nextchromo.append(chromosomes[i][j]+velocity[i][j])
        chromosomes2.append(nextchromo)
    return chromosomes2

def normalize(chromosomes2):
    for l in range(len(chromosomes2)):
        for m in range(len(chromosomes2[l])):
            if chromosomes2[l][m]>0.5:
                chromosomes2[l][m]=1
            else:
                chromosomes2[l][m]=0
    return chromosomes2

def checkpd(chromosomes2):
    personal=[]
    for i in range(len(chromosomes2)):
        personal.append(calaculate_accuracy(data(chromosomes2[i])))
    for j in range(len(personal)):
        if(personal[j]>pb[j]):
            chromosomes[j]=chromosomes2[j]
            pb[j]=personal[j]
    return personal

max(pb)
ind = pb.index(max(pb))
globalbest=chromosomes[ind]
for i in range(20):
    chromosomes2=[]
    personal=[]
    velocity=checkvelocity(globalbest)
    chromosomes2=addingchromosomes(velocity)
    chromosomes2=normalize(chromosomes2)
    personal=checkpd(chromosomes2)
    globalbest=[]
    max(pb)
    ind = pb.index(max(pb))
    globalbest=chromosomes[ind]

max(pb)

ind = pb.index(max(pb))
print(ind)
globalbest=chromosomes[ind]

print(data(globalbest))

X_selected=data(globalbest)
X_selected=df[X_selected]

X_selected

# Split the data into training and testing sets

X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)

from sklearn.linear_model import LogisticRegression


model = XGBClassifier()

model.fit(X_train, y_train)

# Make predictions on the test set

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")

print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 6))


sns.heatmap(
    cm, annot=True, fmt='d', cmap= 'Blues', linewidths=0.4, square=True, cbar=True,
    xticklabels=["N", "Y"],
    yticklabels=["N", "Y"]
)

plt.xlabel('Predicted', fontsize=14, fontweight='bold')
plt.ylabel('Actual', fontsize=14, fontweight='bold')
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.yticks(rotation=360)

plt.show()